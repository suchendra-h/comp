\chapter{Literature Review}

In this chapter we will briefly review some of the literature about the safety of ML methods and identify major research questions in this area. 

In \cite{Amodei}, five major research problems associated with unsafe behavior of ML models is presented. They can be summarized as
\begin{enumerate}
	\item Avoiding Negative Side Effects: How to ensure that the model will not disturb the environment while pursuing its goals, e.g. can a cleaning robot knock over a vase because it can clean faster by doing so? Can we do this without manually specifying everything the robot should not disturb \cite{Amodei}?
	
	\item Avoiding Reward Hacking: How to ensure that the model does not avoid situations to achieve a higher reward. For example, if we reward the robot for achieving an environment free of messes, it might disable its vision so that it won’t find any messes, or cover over messes with materials it can’t see through, or simply hide when humans are around so they can’t tell it about new types of messes \cite{Amodei}. 
	
	\item Scalable Oversight: How to ensure the model respects the parts of the objective function that are expensive to evaluate and makes a safe approximation of these parts. 
	For example, in the cleaning robot example, if the user is happy with the cleaning quality is an expensive objective function, but it can be approximated to presence of any dirt on the floor when the user arrives \cite{Amodei}. 
	
	\item Safe Exploration: How to ensure that the ML model explorations are safe. For example, the robot should experiment with mopping strategies, but putting a wet mop in an electrical outlet is a very bad idea \cite{Amodei}. 
	
	\item Robustness to Distributional Shift: How to ensure that the model performs robustly if the environment shifts from the training environment. For example, strategies a cleaning robot learns for cleaning an office might be dangerous on a factory work-floor \cite{Amodei}.

\end{enumerate}

\section{Machine Learning lifecycle}

To obtain assurance for ML systems it is essential to understand the ML lifecycle and how to analyze safety in each step. In this section we will first introduce these steps and review some of the safety measures for each step. This lifecycle follows a spiral process model, i.e., the stages are iteratively repeated to actively reduce risk \cite{Boehm2000}. ML lifecycle is comprised of four stages \cite{Ashmore2021} 

\subsection{Data Management}
This stage involves collecting, preprocessing, augmenting and initial analysis of data. The training and validation datasets are also prepared in this step.
From assurance perspective, the data collected in this step should be
\begin{itemize}
    \item Relevant
    \item Complete
    \item Balanced
    \item Accurate 
\end{itemize}

\\\textcolor{red}{Write more from \cite{Ashmore2021}}

\subsection{Model Learning}
Selecting the type of the model, hyper-parameters, transfer learning and training the ML model with the train dataset obtained in previous stage.
The final model should be performant, robust, reusable and interpretable. \\\textcolor{red}{Write more from \cite{Ashmore2021}}

\subsection{Model Verification} 
Making sure that the model works well on the data it has not seen before, also known as generalization. If the model fails in this stage, the process will go back to Data Management or Model Learning steps. Model verification involves requirements encoding, test-based verification and formal verification.\\\textcolor{red}{Write more from \cite{Ashmore2021}}

\subsection{Model Deployment} 
Preparing the ML model to be used in the final application. Activities in stage includes integration, monitoring and updating \\\textcolor{red}{Write more from \cite{Ashmore2021}}

The model should be fit for the purpose, tolerable and adaptable.
\section{Open challenges in ML assurance}
