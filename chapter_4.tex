\chapter{Literature Review}
\label{chap:literature}

In this chapter we will briefly review some of the literature about the safety of ML methods and identify major research questions in this area. 

In \cite{Amodei2016}, five major research problems associated with unsafe behavior of ML models is presented. They can be summarized as
\begin{enumerate}
	\item Avoiding Negative Side Effects: How to ensure that the model will not disturb the environment while pursuing its goals, e.g. can a cleaning robot knock over a vase because it can clean faster by doing so? Can we do this without manually specifying everything the robot should not disturb.
	
	\item Avoiding Reward Hacking: How to ensure that the model does not avoid situations to achieve a higher reward. For example, if we reward the robot for achieving an environment free of messes, it might disable its vision so that it won’t find any messes, or cover over messes with materials it can’t see through, or simply hide when humans are around so they can’t tell it about new types of messes.
	
	\item Scalable Oversight: How to ensure the model respects the parts of the objective function that are expensive to evaluate and makes a safe approximation of these parts. 
	For example, in the cleaning robot example, if the user is happy with the cleaning quality is an expensive objective function, but it can be approximated to presence of any dirt on the floor when the user arrives.
	
	\item Safe Exploration: How to ensure that the ML model explorations are safe. For example, the robot should experiment with mopping strategies, but putting a wet mop in an electrical outlet is a very bad idea.
	
	\item Robustness to Distributional Shift: How to ensure that the model performs robustly if the environment shifts from the training environment. For example, strategies a cleaning robot learns for cleaning an office might be dangerous on a factory work-floor.

\end{enumerate}

% Some of the above 
\section{Machine Learning lifecycle}

To obtain assurance for ML systems it is essential to understand the ML lifecycle and how to analyze safety in each step. In this section we will first introduce these steps and review some of the safety measures for each step. This lifecycle follows a spiral process model, i.e., the stages are iteratively repeated to actively reduce risk \cite{Boehm2000}. ML lifecycle is comprised of four stages \cite{Ashmore2021} 

\subsection{Data Management}
This stage involves collecting, preprocessing, augmenting and initial analysis of data. The training and validation datasets are also prepared in this step.
From assurance perspective, the data collected in this step should be
\begin{itemize}
    \item Relevant: The dataset should be relevant to the desired functionality of the final model. For example a dataset of handwritten letters in Japanese language cannot be used for English language. 
    \item Complete: The features of a dataset should not have unintended correlations that can confuse the classifier. For example, if a classifier is trained on pictures of wolves and huskies, and all wolves have snow in the background, it may be concluded that snow in the background means a wolf \cite{Ribeiro2016}. In this case the dataset is not complete because it does not include pictures of wolves with different backgrounds.
    \item Balanced: For classification problems, it can happen that one class has significantly more data-points in the training set than the others and thus, classifier has more exposure to that class. 
    \item Accurate: This property considers factors like sensor accuracy, correctness of data collection and processing method. In the case of supervised learning, labels' accuracy is important. The data collection process should be documented to identify potential inaccuracies \cite{Ashmore2021}.
\end{itemize} 

\subsection{Model Learning}
In this stage of the ML lifecycle, the type of the model and its hyper-parameters are selected. For some ML applications, the dataset is very large or the model structure is complex and therefore, the learning process needs considerable amount of computational power. In these cases, it is reasonable to take advantage of a previously trained model and adapt it to our needs by re-training only the parts that are different from the other application. This process is called \textit{transfer learning} \cite{IanGoodfellow2016}. If there is a need to do transfer learning, it will be decided at this stage and finally the learning process starts using the train dataset obtained in previous stage.
In order to have a clear view of the model in safety related aspects, the final model should be \cite{Ashmore2021}  
\begin{itemize}
    \item Performant: As a requirement for a safer model, it should have a justifiable performance according to the measures introduced in \autoref{chap:ML}.
    \item Robust: The model should be able to perform as well on the unseen data as the training data,i.e., generalizes well to be considered robust. Data augmentation is one of the techniques to increase generalization \cite{Ko2015}. 
    \item Reusable: Using transfer learning can help to use the assurance evidence of the original model, provided that the transfer learning is performed in the right context for the source and destination models. However, reusing models comes with the risk that safety issues propagates to the destination model too 
    \item Interpretable: This property shows how much the decisions made by the model are explainable and thus helps to analyze the safety of such decisions. 
\end{itemize}

\subsection{Model Verification and Validation} 
The black swan problem expounds one of the major challenges in validating ML models. A system or a person could incorrectly conclude from abundance of training data samples that common observations are true \cite{koopman2016challenges}. A model which has only seen white swans, may infer that all swans are white and ignore the fact that there are black swans \cite{taleb2007the}. One major challenge is thus making sure that the model works well, i.e., satisfies its requirements, on the data it has not seen before which is also known as generalization. If the model fails in this stage, the process will go back to Data Management or Model Learning steps. Model verification involves requirements encoding, test-based verification and formal verification. The verification stage should be \cite{Ashmore2021}
\begin{itemize}
    \item Comprehensive: Model verification should ensure that all the requirements of the system and also intended goals of the previous stages of ML lifecycle, i.e., data management and model learning, are covered.
    \item Contextually relevant: Verification process should be relevant to the intended use of the ML model. For example an ML model used in autonomous vehicles, we are more concerned about how changes in the environment will affect model's performance and thus, how robust is the model with changes in weather rather than the changes in image quality. 
    \item Comprehensible: Verification results should be understandable for the users. Requirement violations should be clearly expressed in such a way that the cause of it can be identified and fixed \cite{Ashmore2021}. Ideally, the results should also include any black swan biases present in the model \cite{koopman2016challenges}.
\end{itemize}

\subsection{Model Deployment} 
Preparing the ML model to be used in the final application. Activities in stage includes integration, monitoring and updating. To assure safety of this stage of ML lifecycle, the ML model should have the following properties

\begin{itemize}
    \item Fit-for-Purpose: The difference in hardware can cause performance differences between ML stages. Also, each distinct hardware setting where a model is deployed can affect model's performance. For a model to be fit for purpose, the performance seen in the previous stages should be carried over to the deployment phase. 
    \item Tolerable: The system should be able to tolerate occasional incorrect outputs of the ML model. To accommodate this, the host system should be able to identify the incorrect outputs and to replace them with a safe value so that the system continues the normal processing activities.
    \item Adaptable: Deployed models are in many cases needed to be updated due to variety of reasons including operational, legislative or environmental changes. This property indicates how safe is the process of updating. 
\end{itemize}

\section{Open challenges in ML assurance}
Using an ML component in a system poses several challenges in each step of the ML lifecycle. In the data management step, further research is needed to guarantee security of data and its fitness for the purpose. Although a vast amount of research has been conducted in the model learning stage, there is still a need to further study hyper-parameter selection. In addition, with recent successes in transfer learning, there is still need for more research in assuring safety in this area. Furthermore, safety assurance requires ML models to be reusable and interpretable. Model verification assurance is mainly accomplished using test-based and verifications. However, there is still a need to develop methods to encode model requirements into proper and formal tests. In model deployment stage, there is no explicit equivalent for updating models in software engineering world, therefore, there is a need to devise assurance methods for adaptable safety-critical systems \cite{Ashmore2021}.

In some applications requirements for a safe ML system reinforce each other. For example, accuracy in data management stage will most likely result in more performant model. However, in some cases, there is a trade-off between requirements, an explainable model is probably more exposable to cyberattack \cite{Ashmore2021}. In spite of attempts to address this issue \cite{Johnson2019}, more research is required to adapt these concepts to ML.  